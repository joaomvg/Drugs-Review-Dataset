{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import progressbar\n",
    "from sys import getsizeof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drugs Reviews Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip drugsCom_raw.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    data_train = pd.read_csv('drugsComTrain_raw.tsv', sep='\\t')\n",
    "    data_test = pd.read_csv('drugsComTest_raw.tsv', sep='\\t')\n",
    "\n",
    "    return data_train, data_test\n",
    "\n",
    "data_train,data_test=read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review to Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(review):\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    review=re.sub(r\"&#039;\",\"'\",review) #this particular string was used to replace \"'\"\n",
    "    review = re.sub(r\"[^a-zA-Z0-9]\", \" \", review.lower()) # Convert to lower case\n",
    "    words = review.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = os.path.join(\"./cache\", \"drugreview_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir)  # ensure cache directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #words_train = list(map(review_to_words, data_train))\n",
    "        #words_test = list(map(review_to_words, data_test))\n",
    "        print('Training data:')\n",
    "        words_train=[]\n",
    "        for review in progressbar.progressbar(data_train):\n",
    "            words_train.append(review_to_words(review))\n",
    "        \n",
    "        print('Test data:')\n",
    "        words_test=[]\n",
    "        for review in progressbar.progressbar(data_test):\n",
    "            words_test.append(review_to_words(review))\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = preprocess_data(data_train['review'], data_test['review'], data_train['condition'], data_test['condition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(data, vocab_size = 5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
    "    #       sentence is a list of words.\n",
    "    count=Counter()\n",
    "    for sentence in data:\n",
    "        count.update(sentence)\n",
    "    \n",
    "    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
    "    #       sorted_words[-1] is the least frequently appearing word.\n",
    "    \n",
    "    sorted_words = count.most_common()#this method sorts the words from the most frequent to the less frequent\n",
    "    sorted_words=[word for word,_ in sorted_words]\n",
    "    \n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict=build_dict(train_X_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_train_test():\n",
    "    train_df = data_train[['condition', 'review']]\n",
    "    test_df = data_test[['condition', 'review']]\n",
    "    train_df = train_df.dropna()\n",
    "    test_df = test_df.dropna()\n",
    "\n",
    "    def resub(review):\n",
    "        review = re.sub(r\"&#039;\", \"'\", review)\n",
    "        return review\n",
    "\n",
    "    train_df.review = train_df.review.apply(resub)\n",
    "    test_df.review = test_df.review.apply(resub)\n",
    "\n",
    "    train_df = train_df[~train_df.condition.str.contains('</span>')]\n",
    "    test_df = test_df[~test_df.condition.str.contains('</span>')]\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df=df_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce number of classification items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_conditions(value):\n",
    "    cond = train_df.condition.value_counts() > value\n",
    "\n",
    "    def g(condition):\n",
    "        if cond[condition]:\n",
    "            return condition\n",
    "        else:\n",
    "            return 'other'\n",
    "\n",
    "    train_df['condcopy'] = train_df['condition'].apply(g)\n",
    "\n",
    "    s = set(train_df['condcopy'])\n",
    "    in_s = test_df['condition'].isin(s)\n",
    "    test_df['condcopy'] = test_df['condition']\n",
    "    test_df['condcopy'][~in_s] = 'other'\n",
    "\n",
    "    len_train = len(set(train_df.condcopy))\n",
    "    len_test = len(set(test_df.condcopy))\n",
    "\n",
    "    other_train = train_df.condcopy.value_counts()['other'] / train_df.shape[0] * 100\n",
    "    other_test = test_df.condcopy.value_counts()['other'] / test_df.shape[0] * 100\n",
    "    print('Nr conditions Train: ', len_train, '\\nNr conditions Test: ', len_test)\n",
    "    print('Percentage \"other\", Train: ', other_train, '%')\n",
    "    print('Percentate \"other\", Test: ', other_test, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_conditions(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tknizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(max_features=5000,stop_words=stopwords.words(\"english\"),tokenizer=tknizer):\n",
    "    try:\n",
    "        cv_train=pickle.load(open('cv_train.pkl','rb'))\n",
    "        print('cv_train loaded.')\n",
    "        cv_test=pickle.load(open('cv_test.pkl','rb'))\n",
    "        print('cv_test loaded.')\n",
    "    except:\n",
    "        cv=CountVectorizer(max_features=max_features,stop_words=stop_words,tokenizer=tokenizer)\n",
    "        cv_train=cv.fit_transform(train_df.review)\n",
    "        pickle.dump(cv_train,open('cv_train.pkl','wb'))\n",
    "        print('cv_train saved')\n",
    "        cv_test=cv.transform(test_df.review)\n",
    "        pickle.dump(cv_test,open('cv_test.pkl','wb'))\n",
    "        print('cv_test saved')\n",
    "\n",
    "    return cv_train,cv_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_train,cv_test=cv(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(model, X_train, X_test, y_train, y_test):\n",
    "    preds_train = model.predict(X_train)\n",
    "    acc_train = accuracy_score(preds_train, y_train)\n",
    "    print('accuracy train done.')\n",
    "\n",
    "    preds_test = model.predict(X_test)\n",
    "    acc_test = accuracy_score(preds_test, y_test)\n",
    "    print('accuracy test done.')\n",
    "\n",
    "    print('Train error: ', acc_train, '\\nTest error: ', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MultinomialNB(alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(cv_train,train_df.condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model.predict(cv_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFmodel=RandomForestClassifier(n_estimators=80,random_state=100,verbose=1,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFmodel.fit(cv_train,train_df.condcopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_predict=RFmodel.predict(cv_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'n_estimators':100,'max_depth':10, 'learning_rate':0.1, 'objective':'multi:softmax' ,'verbosity':1,'n_jobs':-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=xgb.XGBClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(cv_train,train_df.condcopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
